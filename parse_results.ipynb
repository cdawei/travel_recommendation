{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Experimental Results & Generate Latex Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, pickle, types\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_suffix = ['Edin', 'Glas', 'Melb', 'Osak', 'Toro']\n",
    "dat_names = ['Edinburgh', 'Glasgow', 'Melbourne', 'Osaka', 'Toronto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methods_all = ['\\\\textsc{Random}', '\\\\textsc{PersTour}', '\\\\textsc{PersTour-L}', '\\\\textsc{PoiPopularity}', \\\n",
    "               '\\\\textsc{PoiRank}', '\\\\textsc{Markov}', '\\\\textsc{MarkovPath}', \\\n",
    "               '\\\\textsc{Rank+Markov}', '\\\\textsc{Rank+MarkovPath}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latex Table for Recommendation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate results filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_fname(dat_ix):\n",
    "    assert(0 <= dat_ix < len(dat_suffix))\n",
    "    \n",
    "    suffix = dat_suffix[dat_ix] + '.pkl'\n",
    "    \n",
    "    frank = os.path.join(data_dir, 'rank-' + suffix)\n",
    "    ftran = os.path.join(data_dir, 'tran-' + suffix)\n",
    "    fcomb = os.path.join(data_dir, 'comb-' + suffix)\n",
    "    frand = os.path.join(data_dir, 'rand-' + suffix)\n",
    "    fijcai = os.path.join(data_dir, 'ijcai-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    return frank, ftran, fcomb, frand, fijcai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the F1 score for recommended trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1(traj_act, traj_rec, noloop=False):\n",
    "    '''Compute recall, precision and F1 for recommended trajectories'''\n",
    "    assert(isinstance(noloop, bool))\n",
    "    assert(len(traj_act) > 0)\n",
    "    assert(len(traj_rec) > 0)\n",
    "    \n",
    "    if noloop == True:\n",
    "        intersize = len(set(traj_act) & set(traj_rec))\n",
    "    else:\n",
    "        match_tags = np.zeros(len(traj_act), dtype=np.bool)\n",
    "        for poi in traj_rec:\n",
    "            for j in range(len(traj_act)):\n",
    "                if match_tags[j] == False and poi == traj_act[j]:\n",
    "                    match_tags[j] = True\n",
    "                    break\n",
    "        intersize = np.nonzero(match_tags)[0].shape[0]\n",
    "        \n",
    "    recall = intersize / len(traj_act)\n",
    "    precision = intersize / len(traj_rec)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the pairs-F1 score for recommended trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef float calc_pairsF1(y, y_hat):\n",
    "    assert(len(y) > 0)\n",
    "    assert(len(y) == len(set(y))) # no loops in y\n",
    "    cdef int n, nr, n0, n0r, nc, poi1, poi2, i, j\n",
    "    n = len(y)\n",
    "    nr = len(y_hat)\n",
    "    n0 = n*(n-1) / 2\n",
    "    n0r = nr*(nr-1) / 2\n",
    "    \n",
    "    # y determines the correct visiting order\n",
    "    order_dict = dict()\n",
    "    for i in range(n):\n",
    "        order_dict[y[i]] = i\n",
    "        \n",
    "    nc = 0\n",
    "    for i in range(nr):\n",
    "        poi1 = y_hat[i]\n",
    "        for j in range(i+1, nr):\n",
    "            poi2 = y_hat[j]\n",
    "            if poi1 in order_dict and poi2 in order_dict and poi1 != poi2:\n",
    "                if order_dict[poi1] < order_dict[poi2]: nc += 1\n",
    "\n",
    "    cdef float precision, recall, F1\n",
    "    precision = (1.0 * nc) / (1.0 * n0r)\n",
    "    recall = (1.0 * nc) / (1.0 * n0)\n",
    "    if nc == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2. * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load results data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_results(dat_ix):\n",
    "    assert(0 <= dat_ix < len(dat_suffix))\n",
    "    \n",
    "    frank, ftran, fcomb, frand, fijcai = gen_fname(dat_ix)\n",
    "    #print(frank)\n",
    "    assert(os.path.exists(frank))\n",
    "    #print(ftran)\n",
    "    assert(os.path.exists(ftran))\n",
    "    #print(fcomb)\n",
    "    assert(os.path.exists(fcomb))\n",
    "    #print(frand)\n",
    "    assert(os.path.exists(frand))\n",
    "    #print(fijcai)\n",
    "    assert(os.path.exists(fijcai))\n",
    "\n",
    "    # load results data\n",
    "    recdict_rank = pickle.load(open(frank, 'rb'))\n",
    "    recdict_tran = pickle.load(open(ftran, 'rb'))\n",
    "    recdict_comb = pickle.load(open(fcomb, 'rb'))\n",
    "    recdict_rand = pickle.load(open(frand, 'rb'))\n",
    "    recdict_ijcai = pickle.load(open(fijcai, 'rb'))\n",
    "    \n",
    "    return recdict_rank, recdict_tran, recdict_comb, recdict_rand, recdict_ijcai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate F1-scores from loaded results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_metrics(recdict_rank, recdict_tran, recdict_comb, recdict_rand, recdict_ijcai, func):\n",
    "    assert(isinstance(func, types.FunctionType) or isinstance(func, types.BuiltinFunctionType))\n",
    "    \n",
    "    # deal with missing values: \n",
    "    # get rid of recommendation that not all method are successful, due to ILP timeout.\n",
    "    assert(np.all(sorted(recdict_rank.keys()) == sorted(recdict_tran.keys())))\n",
    "    assert(np.all(sorted(recdict_rank.keys()) == sorted(recdict_comb.keys())))\n",
    "    \n",
    "    keys_all = sorted(recdict_ijcai.keys() & recdict_rank.keys())\n",
    "    \n",
    "    rank1 = []; rank2 = []\n",
    "    for key in keys_all:\n",
    "        rank1.append(func(recdict_rank[key]['REAL'], recdict_rank[key]['REC_POP']))\n",
    "        rank2.append(func(recdict_rank[key]['REAL'], recdict_rank[key]['REC_FEATURE']))\n",
    "    \n",
    "    tran1 = []; tran2 = []\n",
    "    for key in keys_all:\n",
    "        tran1.append(func(recdict_tran[key]['REAL'], recdict_tran[key]['REC_DP']))\n",
    "        tran2.append(func(recdict_tran[key]['REAL'], recdict_tran[key]['REC_ILP']))\n",
    "\n",
    "    comb1 = []; comb2 = []\n",
    "    for key in keys_all:\n",
    "        comb1.append(func(recdict_comb[key]['REAL'], recdict_comb[key]['REC_DP']))\n",
    "        comb2.append(func(recdict_comb[key]['REAL'], recdict_comb[key]['REC_ILP']))\n",
    "            \n",
    "    rand = []\n",
    "    for key in keys_all:\n",
    "        rand.append(func(recdict_rand[key]['REAL'], recdict_rand[key]['REC_RAND']))\n",
    "    \n",
    "    ijcai05T = []; ijcai05L = []\n",
    "    for key in keys_all:\n",
    "        ijcai05T.append(func(recdict_ijcai[key]['REAL'], recdict_ijcai[key]['REC05T']))\n",
    "        ijcai05L.append(func(recdict_ijcai[key]['REAL'], recdict_ijcai[key]['REC05L']))\n",
    "    \n",
    "    metrics = [rand, ijcai05T, ijcai05L, rank1, rank2, tran1, tran2, comb1, comb2]\n",
    "    means = [np.mean(x) for x in metrics]\n",
    "    stds  = [np.std(x)  for x in metrics]\n",
    "    \n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Latex tables from calculated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_latex_table(mean_df, std_df, ismax_df, ismax2nd_df, title, label):    \n",
    "    strs = []\n",
    "    strs.append('\\\\begin{table*}[t]\\n')\n",
    "    strs.append('\\\\caption{' + title + '}\\n')\n",
    "    strs.append('\\\\label{' + label + '}\\n')\n",
    "    strs.append('\\\\centering\\n')\n",
    "    strs.append('\\\\begin{tabular}{l|' + (mean_df.shape[1])*'c' + '} \\\\hline\\n')\n",
    "    for col in mean_df.columns:\n",
    "        strs.append(' & ' + col)\n",
    "    strs.append(' \\\\\\\\ \\\\hline\\n')\n",
    "    for ix in mean_df.index:\n",
    "        for j in range(mean_df.shape[1]):\n",
    "            if j == 0: strs.append(ix + ' ')\n",
    "            jx = mean_df.columns[j]\n",
    "            strs.append('& $')\n",
    "            if ismax_df.loc[ix, jx] == True: strs.append('\\\\mathbf{')\n",
    "            if ismax2nd_df.loc[ix, jx] == True: strs.append('\\\\mathit{')\n",
    "            strs.append('%.3f' % mean_df.loc[ix, jx] + '\\\\pm' + '%.3f' % std_df.loc[ix, jx])\n",
    "            if ismax_df.loc[ix, jx] == True or ismax2nd_df.loc[ix, jx] == True: strs.append('}')\n",
    "            strs.append('$ ')\n",
    "        strs.append('\\\\\\\\\\n')\n",
    "    strs.append('\\\\hline\\n')\n",
    "    strs.append('\\\\end{tabular}\\n')\n",
    "    strs.append('\\\\end{table*}\\n')\n",
    "    return ''.join(strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate evaluation data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func = calc_F1\n",
    "#func = calc_pairsF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "methods = methods_all.copy() \n",
    "\n",
    "mean_df = pd.DataFrame(data=np.zeros((len(methods), len(dat_names)), dtype=np.float), \\\n",
    "                       columns=dat_names, index=methods)\n",
    "std_df  = pd.DataFrame(data=np.zeros((len(methods), len(dat_names)), dtype=np.float), \\\n",
    "                       columns=dat_names, index=methods)\n",
    "\n",
    "for dat_ix in range(len(dat_suffix)):\n",
    "    recdict_rank, recdict_tran, recdict_comb, recdict_rand, recdict_ijcai = load_results(dat_ix)\n",
    "    means, stds = calc_metrics(recdict_rank, recdict_tran, recdict_comb, recdict_rand, recdict_ijcai, func)\n",
    "    assert(len(means) == len(stds) == len(methods))\n",
    "    mean_df[dat_names[dat_ix]] = means\n",
    "    std_df[dat_names[dat_ix]]  = stds\n",
    "\n",
    "ismax_df = pd.DataFrame(data=np.zeros(mean_df.shape, dtype=np.bool), columns=mean_df.columns, index=mean_df.index)\n",
    "ismax2nd_df = ismax_df.copy()\n",
    "for col in ismax_df.columns:\n",
    "    indices = (-mean_df[col]).argsort().values[:2]\n",
    "    ismax_df.iloc[indices[0]][col] = True\n",
    "    ismax2nd_df.iloc[indices[1]][col] = True\n",
    "\n",
    "if func == calc_F1:\n",
    "    title = '''Performance comparison on five datasets in terms of F$_1$ scores. \n",
    "    The best method for each dataset (i.e., a column) is shown in bold, the second best is shown in italic.'''\n",
    "    label = 'tab:f1'\n",
    "else:\n",
    "    title = '''Performance comparison on five datasets in terms of pairs-F$_1$ scores.\n",
    "    The best method for each dataset (i.e., a column) is shown in bold, the second best is shown in italic.'''\n",
    "    label = 'tab:pairf1'\n",
    "strs = gen_latex_table(mean_df, std_df, ismax_df, ismax2nd_df, title, label)\n",
    "\n",
    "print(strs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
